{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ab53b-1d76-4e96-801f-dbffc34dc3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f9922-fbde-4b54-8137-7f6c11984807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dann_utils import get_vendor_info\n",
    "from dann_utils import get_splits\n",
    "from utils import generate_patient_info, crop_image\n",
    "from utils import preprocess, preprocess_image, inSplit\n",
    "\n",
    "\n",
    "vendor_info = get_vendor_info(\"./data/MnM2/dataset_information.csv\")\n",
    "vendor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73bd65-8d4c-4df4-ba7e-79505b8e253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"dann_preprocessed\"):\n",
    "    os.makedirs(\"dann_preprocessed\")\n",
    "\n",
    "splits = get_splits(vendor_info, os.path.join(\"dann_preprocessed\", \"splits.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e1192-4f08-43bf-9b71-37f77fbfb4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info = generate_patient_info(vendor_info, os.path.join(\"dann_preprocessed\", \"patient_info.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af13de-0706-4c8f-9a20-9cf110483082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spacings = [\n",
    "    patient_info[\"{:03d}_{}\".format(id, axis)][\"spacing\"] for axis in [\"SA\", \"LA\"] for id in (\n",
    "        splits[\"train\"] + splits[\"train\"] + splits[\"val\"]\n",
    "    )\n",
    "]\n",
    "spacing_target = np.percentile(np.vstack(spacings), 50, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a007b73-2e22-4e13-8afe-da33b82e02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"dann_preprocessed/training/\"): os.makedirs(\"dann_preprocessed/training/\")\n",
    "if not os.path.isdir(\"dann_preprocessed/validation/\"): os.makedirs(\"dann_preprocessed/validation/\")\n",
    "if not os.path.isdir(\"dann_preprocessed/soft_validation/\"): os.makedirs(\"dann_preprocessed/soft_validation/\")\n",
    "if not os.path.isdir(\"dann_preprocessed/testing/\"): os.makedirs(\"dann_preprocessed/testing/\")\n",
    "\n",
    "preprocess(\n",
    "    {k:v for k,v in patient_info.items() if inSplit(k, splits[\"train\"])},\n",
    "    spacing_target, \"dann_preprocessed/training/\"\n",
    ")\n",
    "preprocess(\n",
    "    {k:v for k,v in patient_info.items() if inSplit(k, splits[\"val\"])},\n",
    "    spacing_target, \"dann_preprocessed/validation/\"\n",
    ")\n",
    "preprocess(\n",
    "    {k:v for k,v in patient_info.items() if inSplit(k, splits[\"val\"])},\n",
    "    spacing_target, \"dann_preprocessed/soft_validation/\", soft_preprocessing=True\n",
    ")\n",
    "preprocess(\n",
    "    {k:v for k,v in patient_info.items() if inSplit(k, splits[\"test\"])},\n",
    "    spacing_target, \"dann_preprocessed/testing/\", soft_preprocessing=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a134d1b-5c6e-42f4-a693-4cfa59a64e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from baseline_1 import Baseline_1\n",
    "from unet_model import Baseline_DANN\n",
    "from utils import AttrDict\n",
    "from utils import GDLoss, CELoss\n",
    "from utils import device\n",
    "from utils import Validator, Checkpointer\n",
    "from utils import dann_training\n",
    "from dann_loader import DANNDataLoader, DANNAllPatients\n",
    "from utils import BATCH_SIZE, EPOCHS, CKPT\n",
    "from utils import transform_augmentation_downsample, transform\n",
    "from utils import plot_history\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "Model = Baseline_DANN\n",
    "\n",
    "model = nn.ModuleDict([\n",
    "    [axis, Model(\n",
    "        AttrDict(**{\n",
    "            \"lr\": 0.01,\n",
    "            \"functions\": [GDLoss, CELoss]\n",
    "        })\n",
    "    )] for axis in [\"SA\", \"LA\"]\n",
    "]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a4a19-3261-4124-bc19-ebc0154ec919",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ckpts = None\n",
    "if ckpts is not None:\n",
    "    for axis, ckpt in ckpts.items():\n",
    "        _, start = os.path.split(ckpt)\n",
    "        start = int(start.replace(\".pth\", \"\"))\n",
    "        ckpt = torch.load(ckpt)\n",
    "        model[axis].load_state_dict(ckpt[\"M_dann\"])\n",
    "        model[axis].optimizer.load_state_dict(ckpt[\"M_dann_optim\"])\n",
    "else:\n",
    "    start = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873dc8b-fe7f-4e81-a4ca-d81cf238c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validators = {\n",
    "    \"SA\": Validator(5),\n",
    "    \"LA\": Validator(5)\n",
    "}\n",
    "\n",
    "for axis in [\"SA\", \"LA\"]:\n",
    "    dann_training(\n",
    "        model[axis],\n",
    "        range(start, EPOCHS),\n",
    "        torch.utils.data.DataLoader(\n",
    "            DANNAllPatients(\n",
    "                os.path.join(\"dann_preprocessed/training/\", axis),\n",
    "                transform=transform_augmentation_downsample\n",
    "            ),\n",
    "            batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    "        ),\n",
    "        DANNDataLoader(\n",
    "            os.path.join(\"dann_preprocessed/validation/\", axis),\n",
    "            batch_size=BATCH_SIZE, transform=transform\n",
    "        ),\n",
    "        validators[axis],\n",
    "        Checkpointer(os.path.join(CKPT, \"M_dann\", axis))\n",
    "    )\n",
    "\n",
    "    plot_history(validators[axis].get_history(\"val\"), \"M-dann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2948d468-8131-4f76-9d30-de1d6089e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_748/682929221.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  temp = torch.load(ckpt)\n",
      "/tmp/ipykernel_748/682929221.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model[axis].load_state_dict(torch.load(ckpt)[\"M\"])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from unet_model import Baseline_DANN\n",
    "from utils import device\n",
    "from dann_loader import DANNDataLoader\n",
    "from utils import BATCH_SIZE, CKPT\n",
    "from utils import transform\n",
    "from dann_utils import infer_predictions\n",
    "from dann_utils import get_splits\n",
    "from utils import postprocess_predictions, display_results\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "Model = Baseline_DANN\n",
    "\n",
    "model = nn.ModuleDict([\n",
    "    [axis, Model()] for axis in [\"SA\", \"LA\"]\n",
    "]).to(device)\n",
    "\n",
    "for axis in [\"SA\", \"LA\"]:\n",
    "    ckpt = os.path.join(CKPT, \"M_dann\", axis, \"best_000.pth\")\n",
    "    temp = torch.load(ckpt)\n",
    "    model[axis].load_state_dict(torch.load(ckpt)[\"M\"])\n",
    "    model[axis].to(device)\n",
    "    model.eval()\n",
    "\n",
    "    infer_predictions(\n",
    "        os.path.join(\"dann_inference\", axis),\n",
    "        DANNDataLoader(\n",
    "            f\"dann_preprocessed/testing/{axis}\",\n",
    "            batch_size = BATCH_SIZE,\n",
    "            transform = transform,\n",
    "            transform_gt = False\n",
    "        ),\n",
    "        model[axis]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d34fc3-4ab1-47be-9bb6-731a8ce8d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"dann_preprocessed\", \"splits.pkl\"), \"rb\") as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(\"dann_preprocessed\", \"patient_info.pkl\"),'rb') as f:\n",
    "    patient_info = pickle.load(f)\n",
    "\n",
    "spacings = [\n",
    "    patient_info[\"{:03d}_{}\".format(id, axis)][\"spacing\"] for axis in [\"SA\", \"LA\"] for id in (\n",
    "        splits[\"train\"] + splits[\"train\"] + splits[\"val\"]\n",
    "    )\n",
    "]\n",
    "spacing_target = np.percentile(np.vstack(spacings), 50, 0)\n",
    "\n",
    "current_spacing = np.percentile(np.vstack(spacings), 50, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff43225a-1f6d-4ff3-92f0-f020e5a468d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      RV_ED_DC   RV_ED_HD  RV_ES_DC   RV_ES_HD     RV_DC      RV_HD  LV_ED_DC  \\\n",
      "axis                                                                            \n",
      "SA    0.798823  39.658234  0.723117  40.663955  0.760970  40.161095  0.899953   \n",
      "LA    0.855576  16.365214  0.792488  19.870313  0.824032  18.117763  0.865388   \n",
      "\n",
      "       LV_ED_HD  LV_ES_DC   LV_ES_HD     LV_DC      LV_HD  MYO_ED_DC  \\\n",
      "axis                                                                   \n",
      "SA    11.399328  0.862108  11.450422  0.881030  11.424875   0.712162   \n",
      "LA          inf  0.849597        inf  0.857493        inf   0.031715   \n",
      "\n",
      "       MYO_ED_HD  MYO_ES_DC   MYO_ES_HD    MYO_DC      MYO_HD  \n",
      "axis                                                           \n",
      "SA     21.247111   0.758861   22.285677  0.735512   21.766394  \n",
      "LA    164.651470   0.035318  171.330777  0.033516  167.991123  \n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for axis in [\"SA\", \"LA\"]:\n",
    "    results[axis] = postprocess_predictions(\n",
    "        os.path.join(\"dann_inference\", axis),\n",
    "        patient_info,\n",
    "        current_spacing,\n",
    "        os.path.join(\"dann_postprocessed\", axis),\n",
    "    )\n",
    "\n",
    "with open(\"dann_postprocessed/results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results,f)\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dad0fd-bfe1-4f5e-b250-b350420b50d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
